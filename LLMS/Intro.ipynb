{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pueden hacer resumenes, traducir, haer y responder preguntas.\n",
    "* LLama de Meta y Gemini\n",
    "* Basados en arquitecturas de deep learning, comunmente transformers\n",
    "* They are large because ther are usually huge neural networks with mill or bill or parameters trained on enormous amounts of text data\n",
    "* Pretaining:learns patterns and understanding from a large, diverse dataset. This is a very computattionally intensive process, the result is a general purpose model or a foundational model. which can be fine tuned on a smaller domain specific dataset , tailoring it for particular use cases and applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function for loading Hugging Face pipelines\n",
    "from transformers import pipeline\n",
    "\n",
    "review = \"The food was good, but service at the restaurant was a bit slow\"\n",
    "\n",
    "# Load the pipeline for text classification\n",
    "classifier = pipeline(task='text-classification', model=model_name)\n",
    "\n",
    "# Pass the customer review to the model for prediction\n",
    "prediction = classifier(review)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language tasks:\n",
    "* language generation:\n",
    "    * text generation\n",
    "    * code generation\n",
    "* Language understanding:\n",
    "    * Text classification , sentiment analysis\n",
    "    * Text summarization\n",
    "    * languaje translation\n",
    "    * intent recognition: determine purpose behind a text\n",
    "    * Question- answering\n",
    "    * Named entity recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model pipeline for text summarization\n",
    "summarizer = pipeline(task=\"summarization\", model=model_name)\n",
    "\n",
    "# Pass the long text to the model to summarize it\n",
    "outputs = summarizer(long_text, max_length=50)\n",
    "\n",
    "# Access and print the summarized text in the outputs variable\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question-answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model pipeline for question-answering\n",
    "qa_model = pipeline(\"question-answering\")\n",
    "question = \"For how long was the Eiffel Tower the tallest man-made structure in the world?\"\n",
    "\n",
    "# Pass the inputs to the pipeline\n",
    "outputs = qa_model(question=question, context=context)\n",
    "\n",
    "# Access and print the answer\n",
    "print(outputs['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is a transformer?\n",
    "* deep learning architecture for text processing, understanding and generation\n",
    "\n",
    "**Characteristics**\n",
    "* no recurrent architecture\n",
    "* capture long-range dependencies in text\n",
    "* tokens are handled simultaneously\n",
    "* attention mechanisms + positional encoding: can weight the relative importance of different words in a sentence when making inferences, such as predicting the next word in a sequential output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original transformer had two main architectural stacks:\n",
    "* encoder\n",
    "* decoder\n",
    "\n",
    "each of these two stacks contain several replicated high-level layers: encoder layers and decoder layers\n",
    "* inside each high-level layer attention mechanisms are applied followed by feed-forward computations to capture complex semantic pattents and dependencies in text\n",
    "* all without the need for recurrent or convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch transformers:\n",
    "* model dimension: dimensionality of embeddings used to represent inputs, outputs and intermediate info within de model.\n",
    "\n",
    "* attention mechanisms typically have multiple heads performing parallel computations, specializing in capturing different types of text dependencies.(typically a divisor of the model dimension)\n",
    "* the model depth is determined by the number of encoder and decoder layers\n",
    "* the original transformer was desinged as an encoder-decoder architecture for accommodating sequence to sequence tasks, current models are based on either one of three architecture types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **encoder-decoder**: language translation, text summarization. notable models: T5,BART\n",
    "* **encoder-only** text classification, extractive QA. notable models: BERT\n",
    "* **decoder-only** text generation, generative qa. notable models: GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linatobon/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Set transformer model hyperparameters\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "\n",
    "# Create the transformer model and assign hyperparameters\n",
    "model = nn.Transformer(\n",
    "    d_model=d_model,\n",
    "    nhead=n_heads,\n",
    "    num_encoder_layers= num_encoder_layers,\n",
    "    num_decoder_layers= num_decoder_layers\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Este curso sobre LLMs se está poniendo muy interesante\"\n",
    "\n",
    "# Define pipeline for Spanish-to-English translation\n",
    "translator = pipeline(\"translation_es_to_en\", model=model_name)\n",
    "\n",
    "# Translate the input text\n",
    "translations = translator(input_text)\n",
    "\n",
    "# Access the output to print the translated text in English\n",
    "print(translations[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for text generation using the gpt2 model\n",
    "generator = pipeline(\n",
    "'text-generation',\"gpt2\")\n",
    "\n",
    "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\"\n",
    "\n",
    "# Complete the prompt\n",
    "prompt = f\"Customer review:\\n{text}\\n\\nHotel reponse to the customer:\\n{response}\"\n",
    "\n",
    "# Pass the prompt to the model pipeline\n",
    "outputs = generator(prompt, max_length=150, pad_token_id=generator.tokenizer.eos_token_id)\n",
    "\n",
    "# Print the generated text\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention mechanism and positional encoding\n",
    "* revsolved the limitation of RNNs which processed sequences one token at a time, failing to capture long-range relationships\n",
    "\n",
    "* **Self attention:** weights the importance of all tokens in the sequence simultaneously, capturing long-range dependencies.\n",
    "\n",
    "* **Positional econding:** a vector is created to describe the tokens position in the sequence. These values are made unique by using sine and cosine functions. by adding these encodings to the embedding it nows contains the position information needed by attention mechanisms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass an appropriate PyTorch class \n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Initialize the positional encoding matrix\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Calculate and assign position encodings to the matrix\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    # Update the embeddings tensor adding the positional encodings\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-headed self attention\n",
    "\n",
    "Self attention helps transformers understand the interrelationship between words in a sequence\n",
    "\n",
    "* given a sequence of n token embeddings,each embedding is first projected into three matrices of equal dimension: query, key and values.\n",
    "* scaled dot product is a common approach, which applies dot-product (or cosine similatity) between every query-key pair in a sequence, to yield a matrix od attention scores between words.\n",
    "* softmax scaling: produces a matrix of attention weights, indicating the relevance or attention that the model assigns to each token in a sequence\n",
    "\n",
    "* Attention weights aree then multiplied by the values to update token embeddings with relevant information about the sequence.\n",
    "\n",
    "* Transformers parallelize multiple attention heads to learn different semantics aspects of the sequence, like contexts, sentiment and syntax interactions.\n",
    "\n",
    "* MHA concatenates attention head outputs and linearly projects them to keep consistent embedding dimensions.\n",
    "\n",
    "* each head dimension is a divisor of the model dimension.\n",
    "* 3 linear transformations are defined for attention inputs (q,k,v) and 1 for final output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Set the number of attention heads\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // num_heads\n",
    "\t\t# Set up the linear transformations\n",
    "        self.query_linear = nn.Linear(d_model,d_model)\n",
    "        self.key_linear = nn.Linear(d_model,d_model)\n",
    "        self.value_linear = nn.Linear(d_model,d_model)\n",
    "        self.output_linear = nn.Linear(d_model,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(self, x, batch_size):\n",
    "    # Split the sequence embeddings in x across the attention heads\n",
    "    x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "    return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention(self, query, key, mask=None):\n",
    "    # Compute dot-product attention scores\n",
    "    scores = torch.matmul(query, key.permute(1, 2, 0))\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "    # Normalize attention scores into attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, query, key, value, mask=None):\n",
    "    batch_size = query.size(0)\n",
    "\n",
    "    query = self.split_heads(self.query_linear(query), batch_size)\n",
    "    key = self.split_heads(self.key_linear(key), batch_size)\n",
    "    value = self.split_heads(self.value_linear(value), batch_size)\n",
    "\n",
    "    attention_weights = self.compute_attention(query, key, mask)\n",
    "\t\t\n",
    "    # Multiply attention weights by values, concatenate and linearly project outputs\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "    return self.output_linear(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building an encoder transformer\n",
    "\n",
    "* Encoder only transformer: understanding and representing the input data: text classification, NER, intent recognition\n",
    "\n",
    "components:\n",
    "* Transformer body: encoder stack with N encoder layers. each enconder layer was a multi-headed self attention mechanism to capture relationships between elements in the sequence.\n",
    "    * feed-forward (sub) layers: to map this knowledge into abstract nonlinear representations.\n",
    "    * combined with normalizations, skpi connections, droputs\n",
    "* Transformer head: produce task-specific outputs.(classification, regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed forward sublayer: \n",
    "two connected linear layers + ReLU activation\n",
    "\n",
    "1. Fully Connected Layers: The FFN comprises two linear (fully connected) layers that transform the input data. The first layer expands the input dimension from dmodel​=512 to a larger dimension dff​=2048, and the second layer projects it back to dmodel​.\n",
    "\n",
    "2. Activation Function:\n",
    "\n",
    "A Rectified Linear Unit (ReLU) activation function is applied between these two linear layers. This function is defined as ReLU(x)=max(0,x) and is used to introduce non-linearity into the model, helping it to learn more complex patterns.\n",
    "\n",
    "* mask: to prevent processing of padding tokens. Padding ensures equal length across sequences for optimal batch processing by appending speacial padding tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer body encoder: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer head: \n",
    "[medium post](https://medium.com/@bavalpreetsinghh/transformer-from-scratch-using-pytorch-28a5d1b2e033)\n",
    "* classification head: maps the resulting encoder hidden states into class probabilities aided by a softmax function.\n",
    "\n",
    "* regression head: linear layer with an output dimension equal to the number of target regression outputs,output_dim  b1 when predicting a single numerical value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardSubLayer(nn.Module):\n",
    "    # Specify the two linear layers' input and output sizes\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardSubLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model,d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff,d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\t# Apply a forward pass\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the initialization of elements in the encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout =  nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        return self.norm2(x + self.dropout(ff_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
    "        # Define a stack of multiple encoder layers\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\t\n",
    "    # Complete the forward pass method\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        # Add linear layer for multiple-class classification\n",
    "        self.fc = nn.Linear(d_model,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x[:, 0, :])\n",
    "        # Obtain log class probabilities upon raw outputs\n",
    "        return F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "\n",
    "# Instantiate the encoder transformer's body and head\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "classifier = ClassifierHead(d_model, num_classes)\n",
    "\n",
    "# Complete the forward pass \n",
    "output = encoder.forward(input_sequence, mask)\n",
    "classification = classifier.forward(output)\n",
    "print(\"Classification outputs for a batch of \", batch_size, \"sequences:\")\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a decoder transformer\n",
    "\n",
    "* specialized in sequence generation tasks not requiring an encoder. like text generation and completion\n",
    "\n",
    "* **Masked multi head self attention**: helps the model specialize in predicting the next word in a sequence one step at a time.\n",
    "* upper triangular mask: hide future positions in sequence, only attend tokens before current one. each token in the sequence only pays attention to past information\n",
    "\n",
    "* tansformer head consist of a linear layer with softmax activation overthe entire vocabulary to estimate the likelihood of each word or token being the next one to generate and returning the most likely one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_sequence_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        # Add a linear layer (head) for next-word prediction\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, self_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask)\n",
    "\n",
    "        # Apply the forward pass through the model head\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "# Create a triangular attention mask for causal attention\n",
    "self_attention_mask = (1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()  # Upper triangular mask\n",
    "\n",
    "# Instantiate the decoder transformer\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "output = decoder(input_sequence, self_attention_mask)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-decoder transformer\n",
    "\n",
    "* text transalation and summarization\n",
    "\n",
    "* the encoder connects to decoder through cross attention, after the masked attention taking to inputs:\n",
    "1. information processed throughout decoder\n",
    "2. final hidden states from encoder block: look back at processed input sequence and finds out next output token to generate\n",
    "\n",
    "* cross attention identifies key words in sequence to generate the next spanish word\n",
    "* only the final encoder outpust are fed to every layer in the decoder for cross-attention\n",
    "* the models output head consists of a linear layer followed  by a softmax activation, converting decoder outputs into next-word probabilities\n",
    "* the role of the decoder inputs, called output embedding, the decoder only needs to take actual target sequences during training time\n",
    "* words in the target sequence act as training labels during the next word generation process.\n",
    "* at inference time, decoder assumes the role of generating a target sequence, starting with an empty output enbedding and gradually taking as its inputs the target words it is generating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # Initialize the causal (masked) self-attention and cross-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, causal_mask, encoder_output, cross_mask):\n",
    "        # Pass the necessary arguments to the causal self-attention and cross-attention\n",
    "        self_attn_output = self.self_attn(x, x, x, causal_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, cross_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of random input sequences\n",
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "padding_mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "causal_mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1)\n",
    "\n",
    "# Instantiate the two transformer bodies\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "# Pass the necessary masks as arguments to the encoder and the decoder\n",
    "encoder_output = encoder(input_sequence, padding_mask)\n",
    "decoder_output = decoder(input_sequence, padding_mask, encoder_output, causal_mask)\n",
    "print(\"Batch's output shape: \", decoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of random input sequences\n",
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "padding_mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "causal_mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1)\n",
    "\n",
    "# Instantiate the two transformer bodies\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "# Pass the necessary masks as arguments to the encoder and the decoder\n",
    "encoder_output = encoder(input_sequence, padding_mask)\n",
    "decoder_output = decoder(input_sequence, causal_mask, encoder_output, padding_mask)\n",
    "print(\"Batch's output shape: \", decoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize positional encoding layer and stack of EncoderLayer modules\n",
    "class TransformerEncoder(nn.Module):\n",
    "  \n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass the sequence through each layer in the encoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        # Initialize the encoder stack of the Transformer\n",
    "        self.encoder = TransformerEncoder(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        return encoder_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
