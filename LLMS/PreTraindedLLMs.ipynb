{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre trained LLMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pipeline(): automatic model and tokenizer selection\n",
    "* Auto classes(Automodel class)\n",
    "* from_pretrained(): load pretrained model weights and tokenizer as specified in model_name\n",
    "* AutoModel generic class that when given inputs for inference returns the hideen states produced by the model body. lacks a task-specific head, like a classification head, which we can include."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tokenize inputs: enabling padding and sequence truncation when exceeding the max length\n",
    "2. get model hidden states and aggregate them using pooler_output\n",
    "3. hidden states are forward-passed through the custom classification head for binary classification to obtain raw logits that are mapped into class probabilities using softmax\n",
    "\n",
    "AutoModelForSequenceClassification\n",
    "AutoModelForCasualLLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How text generation LLM training works\n",
    "1. requires of examples composed of input-target sequence pairs. \n",
    "2. input sequence represents a segment of text\n",
    "3. target sequence is the same as the input sequence but shifted by one token to the left, ensuring the next token in the original sequence is the prediction target\n",
    "4. with many sequences the llm eventually learns to predict the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "  model_name, num_labels=2)\n",
    "\n",
    "text = [\"The best movie I've ever watched!\", \"What an awful movie. I regret watching it.\"]\n",
    "\n",
    "# Tokenize inputs and pass them to the model for inference\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "predicted_classes = torch.argmax(logits, dim=1).tolist()\n",
    "for idx, predicted_class in enumerate(predicted_classes):\n",
    "    print(f\"Predicted class for \\\"{text[idx]}\\\": {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "\n",
    "# Load the tokenizer and the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "english_inputs = [\"Hello\", \"Thank you\", \"How are you?\", \"Sorry\", \"Goodbye\"]\n",
    "\n",
    "# Encode the inputs, generate translations, decode, and print them\n",
    "for english_input in english_inputs:\n",
    "    input_ids = tokenizer.encode(english_input,return_tensors=\"pt\")\n",
    "    translated_ids = model.generate(input_ids)\n",
    "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"English: {english_input} | Spanish: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs for question answering\n",
    "\n",
    "3 types:\n",
    "* Extractive: the LLM extracts the answer to a question from a provided context. requires  an encoder only architecture\n",
    "* Open Generative QA: model constructs answer using language generation based on the context rather than extracting it. Encoder decoder approach\n",
    "* Closed generative QA:  LLM fully generated the answer no context provided. Decoder only architecture\n",
    "\n",
    "* to accommodate contexts longer tha max sequence length, return_overflowing_tokens=True to activate a sliding window that splits the context into multiple overlapping windows.\n",
    "* max_length and stride set up a window size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a specific subset of the dataset\n",
    "mlqa = load_dataset(\"xtreme\", name=\"MLQA.en.en\")\n",
    "\n",
    "question = mlqa[\"test\"][\"question\"][0]\n",
    "context = mlqa[\"test\"][\"context\"][0]\n",
    "print(\"Question: \", question)\n",
    "print(\"Context: \", context)\n",
    "\n",
    "# Initialize the tokenizer using the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/minilm-uncased-squad2\")\n",
    "\n",
    "# Tokenize the inputs returning the result as tensors\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "print(\"First five encoded tokens: \", inputs[\"input_ids\"][0][:5])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
