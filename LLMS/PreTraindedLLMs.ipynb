{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre trained LLMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pipeline(): automatic model and tokenizer selection\n",
    "* Auto classes(Automodel class)\n",
    "* from_pretrained(): load pretrained model weights and tokenizer as specified in model_name\n",
    "* AutoModel generic class that when given inputs for inference returns the hideen states produced by the model body. lacks a task-specific head, like a classification head, which we can include."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tokenize inputs: enabling padding and sequence truncation when exceeding the max length\n",
    "2. get model hidden states and aggregate them using pooler_output\n",
    "3. hidden states are forward-passed through the custom classification head for binary classification to obtain raw logits that are mapped into class probabilities using softmax\n",
    "\n",
    "AutoModelForSequenceClassification\n",
    "AutoModelForCasualLLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How text generation LLM training works\n",
    "1. requires of examples composed of input-target sequence pairs. \n",
    "2. input sequence represents a segment of text\n",
    "3. target sequence is the same as the input sequence but shifted by one token to the left, ensuring the next token in the original sequence is the prediction target\n",
    "4. with many sequences the llm eventually learns to predict the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "  model_name, num_labels=2)\n",
    "\n",
    "text = [\"The best movie I've ever watched!\", \"What an awful movie. I regret watching it.\"]\n",
    "\n",
    "# Tokenize inputs and pass them to the model for inference\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "predicted_classes = torch.argmax(logits, dim=1).tolist()\n",
    "for idx, predicted_class in enumerate(predicted_classes):\n",
    "    print(f\"Predicted class for \\\"{text[idx]}\\\": {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "\n",
    "# Load the tokenizer and the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "english_inputs = [\"Hello\", \"Thank you\", \"How are you?\", \"Sorry\", \"Goodbye\"]\n",
    "\n",
    "# Encode the inputs, generate translations, decode, and print them\n",
    "for english_input in english_inputs:\n",
    "    input_ids = tokenizer.encode(english_input,return_tensors=\"pt\")\n",
    "    translated_ids = model.generate(input_ids)\n",
    "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"English: {english_input} | Spanish: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs for question answering\n",
    "\n",
    "3 types:\n",
    "* Extractive: the LLM extracts the answer to a question from a provided context. requires  an encoder only architecture\n",
    "* Open Generative QA: model constructs answer using language generation based on the context rather than extracting it. Encoder decoder approach\n",
    "* Closed generative QA:  LLM fully generated the answer no context provided. Decoder only architecture\n",
    "\n",
    "* to accommodate contexts longer tha max sequence length, return_overflowing_tokens=True to activate a sliding window that splits the context into multiple overlapping windows.\n",
    "* max_length and stride set up a window size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a specific subset of the dataset\n",
    "mlqa = load_dataset(\"xtreme\", name=\"MLQA.en.en\")\n",
    "\n",
    "question = mlqa[\"test\"][\"question\"][0]\n",
    "context = mlqa[\"test\"][\"context\"][0]\n",
    "print(\"Question: \", question)\n",
    "print(\"Context: \", context)\n",
    "\n",
    "# Initialize the tokenizer using the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/minilm-uncased-squad2\")\n",
    "\n",
    "# Tokenize the inputs returning the result as tensors\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "print(\"First five encoded tokens: \", inputs[\"input_ids\"][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM upon the model checkpoint\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckp)\n",
    "\n",
    "with torch.no_grad():\n",
    "  # Forward-pass the input through the model\n",
    "  outputs = model(**inputs)\n",
    "\n",
    "# Get the most likely start and end answer position from the raw LLM outputs\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "# Access the tokenized inputs tensor to get the answer span\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "\n",
    "# Decode the answer span to get the extracted answer text\n",
    "answer = tokenizer.decode(answer_span)\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tunning and transfer learning\n",
    "\n",
    "taking a pre-trained model and retraining it with domain specific data. Fine tunning approaches:\n",
    "* full fine tuning: updating weights across the entire model (computationally expensive)\n",
    "* partial fine tuning: weights in lower layers of the model body responsible for capturing general language understanding remain fixed, updating task-specific layers in the model head only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "Adapts a previously trained model on one task to a differnete but related task. leverages knowledge gained in one domain to enhance performance in another related domain.\n",
    "\n",
    "* Zero shot learning:  perform tasks never seen during training\n",
    "* One shot, few shot learning: adapt a model to a new task with one or few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained LLM, specifying its use for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Set up training arguments with a batch size of 8 per GPU and 5 epochs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./smaller_bert_finetuned\", per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    ")\n",
    "# Set up trainer, assigning previously set up training arguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer and assign a training and validation set to it\n",
    "trainer = Trainer(model=model, args=training_args,\n",
    "    \t\t\tcompute_metrics=compute_metrics,\n",
    "    \t\t\ttrain_dataset=topics_encoded[\"train_random\"],\n",
    "    \t\t\teval_dataset=topics_encoded[\"validation_random\"],\n",
    "    \t\t\ttokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Training loop to fine-tune the model\n",
    "# trainer.train()\n",
    "\n",
    "input_texts = [\"It's dark and rainy outside\", \"I love penguins!\"]\n",
    "\n",
    "# Tokenize the input sequences and pass them to the model\n",
    "inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtain class labels from raw predictions\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "for i, predicted_label in enumerate(predicted_labels):\n",
    "    print(f\"\\n Input Text {i + 1}: {input_texts[i]}\")\n",
    "    print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the four input texts (without labels) to the pipeline\n",
    "predictions = sentiment_analysis([example[\"text\"] for example in test_examples])\n",
    "\n",
    "true_labels = [example[\"label\"] for example in test_examples]\n",
    "predicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n",
    "\n",
    "# Load the accuracy metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "result = accuracy.compute(references=true_labels, predictions=predicted_labels)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
