{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arboles y bosques aleatorios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "es una estructura de datos que se puede utilizar para establecer un conjunto de reglas de decision.\n",
    "\n",
    "Esta conformado por:\n",
    "* Nodos: puntos a partir del los cuales van saliendo las ramas, donde se va ramificando\n",
    "* Nodo raiz: punto desde donde se origina el arbol\n",
    "* Nodo hoja: ninguna rama se desprende del nodo\n",
    "* A excepcion del nodo terminal , todos los nodos representan una úncia variable y cada una de las ramas representan las posibles categorias (valores que puede tomar la variable)\n",
    "* Nodo terminal: decision final del algoritmo, valor que va a devolver al final de la ejecución\n",
    "* Ramas\n",
    "* Hojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Homogeneidad: intenta identificar una variable y establecer una clasificación dependiendo de una serie de valores, para dar al final una distribución lo mas homogenea posible en lo que se tata de referenciarse a la variable objetivo.\n",
    "una distribución homogenea significa que valores similares de la variable objetivo tienen que ser agrupados conjuntamente, de modo que una decision concreta pueda ser llevada a cabo.\n",
    "\n",
    "* Identificar una variable que resulte lo mas homogenea posible en la  clasificacion (entropía y ganancia/perdida de la informacion, indice de Gini,reduccion de la varianza)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropia: \n",
    "utiliza la teoria de la informacion, medida de incertidumbre. **La premisa es que como mas homogeneo o mas puros son los nodos de un abol, se requiere menos informacion para representarlos.** es la heterogeneidad de un nodo\n",
    "desde la teoria de la informacion, es el numero minimo de bits que hacen falta para codificar una determinada clasificacion de un miembro arbitario de un conjunto fijado a priori.\n",
    "cualquier reduccion de una entropia de un modo no deseable se medira como una ganancia de informacion. \n",
    "\n",
    "en el caso de un arbol de decision, los nodos que cuando los añadimos como resultado resultan en mayor ganancia de informacion del arbol global, se añade a la configuración. \n",
    "\n",
    "Separabilidad de la informacion\n",
    "\n",
    "\n",
    "H(S) = \\sum -p{i}log_{2}(p{i})\n",
    "(suma de las probabiliddaes de todos los valores que puede tomar la variable objetivo, la entropia siempre es positiva porque el logaritmo en base 2 de un numero mas pequeño que 2 es negativo )\n",
    "\n",
    "* la entropia es cero cuando todas las observaciones fueron perfectamente homogeneas\n",
    "* toma su maximo(logaritmo en base 2 de l numero de categorias) cuando las observaciones son heterogeneas, cuando todos los pi son iguales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* si tenemos una entropia de cero, todas las observaciones estarían en una sola categoria.\n",
    "* el objetivo es ir dividiendo y clasificando los datos hasta conseguir un nodo hoja que tenga entropía cero.Donde ya no poder clasificar mas alla porque ya no ganamos mas informacion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ganancia de informacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indica cuánta información nos brinda una característica particular o una variable particular sobre los resultados finales.reduccion de la entropia a medida que vamos agregando mas variables o información al sistema.\n",
    "\n",
    "Se calcula como la entropia original o en un nodo intermedio y a partir de aqui elegiriamos una variable que v que puede tomar cualquier valor dentro del conjunto de categorias. A esta entropia original le restamos el sumatorio sobre todas las categorias que puede tomar como valor la variable V de la proporcion del datos donde la variable V toma la categoria c sobre la proporcion total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmos para crear arboles de decision\n",
    "\n",
    "#### Algoritmo ID3\n",
    "\n",
    "1. calculamos la entropia inicial del sistema basandonos en la variable objetivo a predecir\n",
    "2. calcular ganancia de informacion para cada variable candidata para un nodo. seleccioando la variable que nos da máxima ganancia de infromacion como nodo de decision\n",
    "3. repetimos el paso 2 para cada rama de cada nodo(variable candidata). el nuevo nodo identificado es un nodo hoja\n",
    "4. comprobamos si el nodo hoja clasifica correctamente los datos. Si es así paramos con esa rama. si no es asín volvemos al paso 2 e iteramos para ramificarlo.\n",
    "\n",
    "paramos cuando el nodo resultante es totalmente homogeneo, tenemos una entropía nula. para evitar overfitting definimos la profundidad maxima (el maximo numero de divisiones que tendra cualquier rama del arbol ) o el numero de observacione minimo que debe tener un nodo para proceder a dividirlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otros algoritmos\n",
    "\n",
    "* indice de gini: variable objetivo binaria\n",
    "* detector automatico de la interaccion con chi cuadrado (CHAID): Encuentra significancia estadistica entre un nodo padre y un nodo hijo\n",
    "* reducción de la varianza: se calcula la varianza para todos los nodos y lleva a cabo una media ponderada de la varianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La poda del arbol\n",
    "\n",
    "permitir que el arbol crezca hasta que los nodos tengan un numero minimo de distancia entre ellos y luego eliminar las ramas o nodos que no nos proporcionan demasiado poder de clasificacion, este proceso se llama la poda del arbol y se puede hacer de abajo a arriba o de arriba a abajo.\n",
    "\n",
    "* Reducción de error en la poda: metodo de arriba a abajo bastante sencillo. consistente en tormar todos y cada uno de los nodos terminales y reemplazar cada nodo por la categoria mas popular \n",
    "\n",
    "* Poda del coste de complejidad: se trata de ir generando subconjuntos de arboles, donde T0 es el arbol sin podar seria el abrol que ha crecido lo maximo posible y el caso mas extremo TN es podar tanto hasta llegar al nodo raiz.\n",
    "1. Definir tasa de error para un arbol T en un datasest D err(T,D)\n",
    "2. El numero de nodos terminales en el arbol viene dado por leaves(T). Denotamos por prune(T,t) el subarbol t que resulta de recortar el arbol T.\n",
    "3. Definimos la funcion M , error que produce la poda, como el cociente M = (err(prune(T,t),D)- err(T,D))/ abs(leaves(T)-abs(leabes(prune(T,t))))\n",
    "4. Calculamos M para todos los subarboles de T y el subarbol que minimice el cocience M seria eliminado; ya que su poda no conllevaria a aumentos en el error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables numericas continuas o NA\n",
    "\n",
    "* En caso de variables continuas hay que encontrar umbrales óptimos de corte para convertirlo a categorías.\n",
    "\n",
    "cómo?\n",
    "1. ordenamos los datos numericos de forma ascendente\n",
    "2. marcamos los rangos de transición de una categoría a otra de la predicción\n",
    "3. calculamos el punto medio de cada umbral de cambio(donde pasa de una categoría a otra)\n",
    "\n",
    "\n",
    "* en el caso de tener NA podemos asignar el valor mas frecuente de la columna\n",
    "* en el caso de tener NA podemos asignar el valor mas frecuente de la columna , para su categoría\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
